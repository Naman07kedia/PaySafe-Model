{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cddba525-4701-4a5d-947e-67e1632756be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior profiling features added: ['amount_z_orig', 'amount_dev_dest', 'night_dev_orig', 'burst_dev', 'BehaviorRisk']\n",
      "New feature matrix shape: (6362620, 16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assumes df includes engineered features from Step 2:\n",
    "# ['step','type','amount','Day','Hour','IsNight','LogAmount',\n",
    "#  'DailyTxnCount','OrigTxnCount','DestTxnCount','TxnCountWindow',\n",
    "#  'RuleHighValue','RuleRapidFire','Type_encoded','isFraud']\n",
    "# and that Step 3 produced X, y, X_train, X_test, y_train, y_test, hybrid_score, y_pred_alert\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Baseline profiles (origin and destination)\n",
    "# -------------------------------\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\naman\\\\PaySafe UPI Fraud Detection\\\\Data\\\\cleaned_Data.csv\")\n",
    "\n",
    "# Origin (nameOrig) baselines\n",
    "orig_profile = df.groupby('nameOrig').agg(\n",
    "    mean_amount=('amount', 'mean'),\n",
    "    std_amount=('amount', 'std'),\n",
    "    median_amount=('amount', 'median'),\n",
    "    mean_hour=('Hour', 'mean'),\n",
    "    night_rate=('IsNight', 'mean'),\n",
    "    txn_per_day_mean=('DailyTxnCount', 'mean'),\n",
    "    txn_per_day_std=('DailyTxnCount', 'std'),\n",
    ").fillna(0).reset_index()\n",
    "\n",
    "# Destination (nameDest) baselines\n",
    "dest_profile = df.groupby('nameDest').agg(\n",
    "    dest_mean_amount=('amount', 'mean'),\n",
    "    dest_std_amount=('amount', 'std'),\n",
    "    dest_median_amount=('amount', 'median'),\n",
    "    dest_night_rate=('IsNight', 'mean'),\n",
    "    dest_txn_count=('DestTxnCount', 'mean'),\n",
    ").fillna(0).reset_index()\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Attach profiles to the full dataframe\n",
    "# -------------------------------\n",
    "df_bp = df.merge(orig_profile, on='nameOrig', how='left').merge(dest_profile, on='nameDest', how='left')\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Deviation and risk signals\n",
    "# -------------------------------\n",
    "# Z-score deviation from origin baseline\n",
    "df_bp['amount_z_orig'] = (df_bp['amount'] - df_bp['mean_amount']) / (df_bp['std_amount'] + 1e-9)\n",
    "\n",
    "# Robust deviation from destination median\n",
    "df_bp['amount_dev_dest'] = (df_bp['amount'] - df_bp['dest_median_amount']) / (df_bp['dest_std_amount'] + 1e-9)\n",
    "\n",
    "# Night behavior deviation\n",
    "df_bp['night_dev_orig'] = df_bp['IsNight'] - df_bp['night_rate']\n",
    "\n",
    "# Burstiness vs personal norm\n",
    "df_bp['burst_dev'] = (df_bp['TxnCountWindow'] - df_bp['txn_per_day_mean']) / (df_bp['txn_per_day_std'] + 1e-9)\n",
    "\n",
    "# Simple behavior risk score (bounded 0â€“1)\n",
    "risk_components = (\n",
    "    np.clip(np.abs(df_bp['amount_z_orig']), 0, 5)/5 +\n",
    "    np.clip(np.abs(df_bp['amount_dev_dest']), 0, 5)/5 +\n",
    "    np.clip(np.abs(df_bp['burst_dev']), 0, 5)/5 +\n",
    "    df_bp['RuleHighValue']*0.5 +\n",
    "    df_bp['RuleRapidFire']*0.5\n",
    ")\n",
    "df_bp['BehaviorRisk'] = np.clip(risk_components / 3.0, 0, 1)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Prepare behavior features for modeling\n",
    "# -------------------------------\n",
    "\n",
    "# Recreate X and y if not already defined\n",
    "feature_cols = [\n",
    "    'amount','LogAmount','Hour','IsNight','DailyTxnCount',\n",
    "    'OrigTxnCount','DestTxnCount','TxnCountWindow',\n",
    "    'RuleHighValue','RuleRapidFire','Type_encoded'\n",
    "]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df['isFraud'].astype(int)\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "behavior_features = [\n",
    "    'amount_z_orig', 'amount_dev_dest', 'night_dev_orig', 'burst_dev', 'BehaviorRisk'\n",
    "]\n",
    "\n",
    "# Align with X indices to avoid leakage/misalignment\n",
    "bp_for_X = df_bp.loc[X.index, behavior_features].copy()\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Extend feature matrix with behavior profiling\n",
    "# -------------------------------\n",
    "X_bp = pd.concat([X, bp_for_X], axis=1)\n",
    "\n",
    "print(\"Behavior profiling features added:\", behavior_features)\n",
    "print(\"New feature matrix shape:\", X_bp.shape)\n",
    "\n",
    "# Optional: if you want to re-train with behavior features\n",
    "# Re-split to match earlier train/test indices\n",
    "X_train_bp = X_bp.loc[X_train.index]\n",
    "X_test_bp  = X_bp.loc[X_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50bd1c3-c80e-4156-ba3b-b054a87cf2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Scale only the new numeric behavior features if needed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "bp_numeric = ['amount_z_orig','amount_dev_dest','night_dev_orig','burst_dev','BehaviorRisk']\n",
    "scaler_bp = StandardScaler()\n",
    "X_train_bp[bp_numeric] = scaler_bp.fit_transform(X_train_bp[bp_numeric])\n",
    "X_test_bp[bp_numeric]  = scaler_bp.transform(X_test_bp[bp_numeric])\n",
    "\n",
    "# Isolation Forest on extended features\n",
    "iso_bp = IsolationForest(\n",
    "    contamination=0.01,\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ").fit(X_train_bp)\n",
    "\n",
    "# XGBoost with class imbalance handling\n",
    "pos = y_train.sum(); neg = len(y_train) - pos\n",
    "scale_pos_weight = max(1.0, neg / max(1, pos))\n",
    "\n",
    "xgb_bp = XGBClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ").fit(X_train_bp, y_train)\n",
    "\n",
    "# Scores\n",
    "xgb_prob_test_bp = xgb_bp.predict_proba(X_test_bp)[:, 1]\n",
    "iso_score_raw_bp = -iso_bp.decision_function(X_test_bp)\n",
    "iso_score_bp = (iso_score_raw_bp - iso_score_raw_bp.min()) / (iso_score_raw_bp.max() - iso_score_raw_bp.min() + 1e-9)\n",
    "\n",
    "# Rule boost from existing columns\n",
    "rule_boost_bp = 0.5*X_test_bp['RuleHighValue'].values + 0.5*X_test_bp['RuleRapidFire'].values\n",
    "rule_boost_bp = np.clip(rule_boost_bp, 0, 1)\n",
    "\n",
    "hybrid_score_bp = 0.6*xgb_prob_test_bp + 0.3*iso_score_bp + 0.1*rule_boost_bp\n",
    "y_pred_alert_bp = (hybrid_score_bp >= 0.8).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ccb17-1196-419c-bc85-07ddcedc7bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TreeExplainer for the retrained XGBoost\n",
    "explainer_bp = shap.TreeExplainer(xgb_bp)\n",
    "shap_values_bp = explainer_bp.shap_values(X_test_bp)\n",
    "\n",
    "# Global importance plots\n",
    "shap.summary_plot(shap_values_bp, X_test_bp, plot_type=\"bar\")\n",
    "shap.summary_plot(shap_values_bp, X_test_bp)\n",
    "\n",
    "# Save dashboard CSVs\n",
    "# 1) Ranked global importance\n",
    "shap_importance_bp = np.abs(shap_values_bp).mean(axis=0)\n",
    "importance_bp_df = pd.DataFrame({\n",
    "    \"Feature\": X_test_bp.columns,\n",
    "    \"MeanAbsSHAP\": shap_importance_bp\n",
    "}).sort_values(\"MeanAbsSHAP\", ascending=False)\n",
    "importance_bp_df.to_csv(\"C:/Users/naman/PaySafe UPI Fraud Detection/Models/shap_feature_importance_bp.csv\", index=False)\n",
    "\n",
    "# 2) Transaction-level SHAP with outcomes\n",
    "shap_tx_bp = pd.DataFrame(shap_values_bp, columns=X_test_bp.columns, index=X_test_bp.index)\n",
    "shap_tx_bp['HybridScore'] = hybrid_score_bp\n",
    "shap_tx_bp['IsFraud'] = y_test.values\n",
    "shap_tx_bp['Alert'] = y_pred_alert_bp\n",
    "shap_tx_bp.to_csv(\"C:/Users/naman/PaySafe UPI Fraud Detection/Dashboard/shap_transaction_values_bp.csv\", index=False)\n",
    "\n",
    "# 3) Decision Plot for Transparency\n",
    "shap.decision_plot(\n",
    "    explainer.expected_value,\n",
    "    shap_values[0,:],\n",
    "    X_test.columns,\n",
    "    feature_display_range=slice(None, 10)  # top 10 features\n",
    ")\n",
    "\n",
    "# 4) Fraud vs non-fraud averages\n",
    "fraud_mean = shap_tx_bp[shap_tx_bp['IsFraud']==1][X_test_bp.columns].mean()\n",
    "nonfraud_mean = shap_tx_bp[shap_tx_bp['IsFraud']==0][X_test_bp.columns].mean()\n",
    "comparison_bp = pd.DataFrame({\"FraudMeanSHAP\": fraud_mean, \"NonFraudMeanSHAP\": nonfraud_mean})\n",
    "comparison_bp.to_csv(\"C:/Users/naman/PaySafe UPI Fraud Detection/Dashboard/shap_fraud_vs_nonfraud_bp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6388846-1b78-4e16-b8f0-5a6d68ec7040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
